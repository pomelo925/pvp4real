# ============================================================
# PVP4Real Online (HITL) Training Configuration
# ============================================================

# ROS2 topics
ros2_topics:
  rgb: "/camera/camera/color/image_raw"
  depth: "/camera/camera/aligned_depth_to_color/image_raw"
  is_teleop: "/stretch/is_teleop"
  cmd_vel_teleop: "/stretch/cmd_vel_teleop"
  cmd_vel: "/stretch/cmd_vel"

# Common observation / control parameters
common:
  hz: 5.0
  max_lin: 2.0         # 線速度上限 2 m/s
  max_ang: 1.5708      # 角速度上限 90 deg/s (約 1.5708 rad/s)
  depth_max_m: 5.0
  stack_n: 5
  resize:
    height: 84
    width: 84
  seed: 0
  device: "auto"

# Training settings (pvp.hitl.py)
training:
  # is_resume_training: False → train from scratch
  # is_resume_training: True  → must provide checkpoint.resume_from (+ optionally buffer.resume_from)
  is_resume_training: true

  total_steps: 50000
  learning_starts: 500
  batch_size: 128
  log_interval: 10

  # Set true to save Q-value gap analytics (Qbehavior, Qnovice, gap折線圖)
  save_analytic_graph: false

  checkpoint:
    is_saved: true
    save_every: 100          # steps
    saved_model_path: "/workspace/pvp4real/models/online/"
    resume_from: "/workspace/pvp4real/models/offline/0001/chkpt-15000f.zip"        # path to .zip file, required when is_resume_training=true
    overwrite: true
    # Optional: specify full checkpoint file path (overrides saved_model_path)
    save_chkpt_path:     # e.g. "models/online/0001/chkpt-250.zip"

  buffer:
    size: 10000              # capacity for each of human_buffer & replay_buffer
    save_every: 100          # steps
    saved_buffer_path: "/workspace/pvp4real/models/online/"
    resume_from: "/workspace/pvp4real/models/offline/0001/buffer_human-15000f.pkl"        # optional
    overwrite: true
    # Optional: specify full buffer file path (overrides saved_buffer_path)
    save_buffer_human_path: null   # e.g. "models/online/0001/buffer_human-250.pkl"
    save_buffer_replay_path: null  # e.g. "models/online/0001/buffer_replay-250.pkl"

  pvp:
    q_value_bound: 1.0
    bc_loss_weight: 1.0
    with_human_proxy_value_loss: true
    with_agent_proxy_value_loss: true
    only_bc_loss: false
    add_bc_loss: true
    # agent hyperparameters
    gamma: 0.99
    tau: 0.05
    learning_rate: 0.0001
    train_freq: 1          # steps between each gradient update
    gradient_steps: 1      # gradient steps per update



