# ============================================================
# PVP4Real Online (HITL) Training Configuration
# ============================================================

# ROS2 topics
ros2_topics:
  rgb: "/camera/camera/color/image_raw"
  depth: "/camera/camera/aligned_depth_to_color/image_raw"
  is_teleop: "/stretch/is_teleop"
  cmd_vel_teleop: "/stretch/cmd_vel_teleop"
  cmd_vel: "/stretch/cmd_vel"

# Common observation / control parameters
common:
  hz: 5.0
  max_lin: 0.4
  max_ang: 1.2
  depth_max_m: 5.0
  stack_n: 5
  resize:
    height: 84
    width: 84
  seed: 0
  device: "auto"

# Training settings (pvp.hitl.py)
training:
  # is_resume_training: False → train from scratch
  # is_resume_training: True  → must provide checkpoint.resume_from (+ optionally buffer.resume_from)
  is_resume_training: false

  total_steps: 50000
  learning_starts: 500
  batch_size: 128
  log_interval: 10

  # Set true to save Q-value gap analytics (Qbehavior, Qnovice, gap折線圖)
  save_analytic_graph: false

  checkpoint:
    is_saved: true
    save_every: 100          # steps
    saved_model_path: "models/online/"
    resume_from: null        # path to .zip file, required when is_resume_training=true
    overwrite: true

  buffer:
    size: 10000              # capacity for each of human_buffer & replay_buffer
    save_every: 100          # steps
    saved_buffer_path: "models/online/"
    resume_from: null        # path to run dir (e.g., models/online/0002), optional
    overwrite: true

  pvp:
    q_value_bound: 1.0
    bc_loss_weight: 1.0
    with_human_proxy_value_loss: true
    with_agent_proxy_value_loss: true
    only_bc_loss: false
    add_bc_loss: true
    # agent hyperparameters
    gamma: 0.99
    tau: 0.05
    learning_rate: 0.0001
    train_freq: 1          # steps between each gradient update
    gradient_steps: 1      # gradient steps per update



