# ============================================================
# PVP4Real Offline Training Configuration — SCRATCH
# ============================================================
#
# 用途：第一次從頭訓練，資料來源為全 human intervene bag。
#   Loss 設定：BC + Human Proxy Value Loss
#   - only_bc_loss: false
#   - add_bc_loss: true              → actor 模仿 human action
#   - with_human_proxy_value_loss: true  → Q(obs, teleop) → +q_value_bound
#   - with_agent_proxy_value_loss: false → 關閉（novice==behavior，會梯度衝突）
#
# Resume 訓練請使用 config.resume.yaml。
# ============================================================

# ROS2 topics
ros2_topics:
  rgb: "/camera/camera/color/image_raw"
  depth: "/camera/camera/aligned_depth_to_color/image_raw"
  is_teleop: "/stretch/is_teleop"
  cmd_vel_teleop: "/stretch/cmd_vel_teleop"
  cmd_vel: "/stretch/cmd_vel"

# Common observation / control parameters
common:
  hz: 5.0               # control frequency (should match record.frequency)
  max_lin: 0.4          # maps action=+1 to 0.4 m/s
  max_ang: 1.2          # maps action=+1 to 1.2 rad/s
  depth_max_m: 5.0      # depth clip maximum, mapped to 255
  stack_n: 5            # number of frames to stack
  resize:
    height: 84
    width: 84
  seed: 0
  device: "auto"        # auto / cuda / cpu

# Training settings (train.py)
training:
  # is_resume_training: False → read bag + train from scratch
  # is_resume_training: True  → must provide checkpoint.resume_from (+ optionally buffer.resume_from)
  is_resume_training: false

  # Path to the decompressed bag directory (relative to pvp4real/pvp4real/, or absolute).
  # e.g. "datasets/offline/0001/bag"
  # null → auto-detect the latest <dataset_base_path>/<run>/bag.
  bag_path: "datasets/offline/0001/bag"
  dataset_base_path: "datasets/offline/"   # used for bag auto-detection when bag_path is null

  total_steps: 50000
  learning_starts: 500
  batch_size: 128
  log_interval: 10

  # Set true to save Q-value gap analytics (Qbehavior, Qnovice, gap折線圖)
  save_analytic_graph: false

  checkpoint:
    is_saved: true
    save_every: 100          # steps
    saved_model_path: "models/offline/"
    resume_from: null        # e.g. "models/offline/0002/chkpt-5000.zip", required when is_resume_training=true
    overwrite: true          # overwrite existing file with same serial number

  buffer:
    size: 10000              # capacity for each of human_buffer & replay_buffer
    save_every: 100          # steps
    saved_buffer_path: "models/offline/"
    resume_from: null        # e.g. "models/offline/0002", optional
    overwrite: true

  pvp:
    q_value_bound: 1.0
    bc_loss_weight: 1.0
    with_human_proxy_value_loss: true   # push Q(obs, teleop_action) → +q_value_bound
    with_agent_proxy_value_loss: false  # OFF: novice==behavior in offline data → gradient conflict
    only_bc_loss: false
    add_bc_loss: true
    gamma: 0.99
    tau: 0.05
    learning_rate: 0.0001
    train_freq: 1
    gradient_steps: 1
